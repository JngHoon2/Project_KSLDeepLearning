{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "    Running setup.py install for pytorch: started\n",
      "    Running setup.py install for pytorch: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Anaconda3\\envs\\handproj\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-wheel-48l1rhj_'\n",
      "       cwd: C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-install-_s3ejkg6\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\n",
      "  Complete output (5 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-install-_s3ejkg6\\pytorch_0e6a0aec2b664723876c495c393c32e0\\setup.py\", line 15, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Anaconda3\\envs\\handproj\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-record-i6tqkfjb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Anaconda3\\envs\\handproj\\Include\\pytorch'\n",
      "         cwd: C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-install-_s3ejkg6\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\n",
      "    Complete output (5 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-install-_s3ejkg6\\pytorch_0e6a0aec2b664723876c495c393c32e0\\setup.py\", line 11, in <module>\n",
      "        raise Exception(message)\n",
      "    Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Anaconda3\\envs\\handproj\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\1woo0\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_s3ejkg6\\\\pytorch_0e6a0aec2b664723876c495c393c32e0\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\1woo0\\AppData\\Local\\Temp\\pip-record-i6tqkfjb\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Anaconda3\\envs\\handproj\\Include\\pytorch' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Temporal Segment Networks (TSN)\n",
    "\n",
    "- 비디오 기반 인간 행동 인식을 위한 프레임 워크\n",
    "- 한 비디오의 여러 세그먼테에서 종단 간 방식으로 학습하여 장거리 시간 역학을 효과적으로 모델링한다.\n",
    "- 동작 인식을 위해 워프흐름과 RGB차이라는 두 가지 새로운 양식이 도입되었습니다.\n",
    "- TSN은 UCF101 및 HMDB51 벤치마크에서 새로운 최첨단 성능을 확립했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cf13adf7d18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "class VideoRecord(object):\n",
    "    def _init_(self, row):\n",
    "        self._data = row\n",
    "        \n",
    "    @property\n",
    "    def path(self):\n",
    "        return self._data[0]\n",
    "    \n",
    "    @property\n",
    "    def num_frames(self):\n",
    "        return int(self._data[1])\n",
    "    \n",
    "    @property\n",
    "    def label(self):\n",
    "        return int(self._data[2])\n",
    "    \n",
    "class TSNDataSet(data.Dataset):\n",
    "    def _init_(self, root_path, list_file,\n",
    "              num_segments=3, new_length=1, modality='RGB',\n",
    "              image_tmpl='img_{:05d}.jpg', transform=None,\n",
    "              force_grayscale=False, random_shift=True, test_mode=False):\n",
    "        \n",
    "        self.root_path = root_path\n",
    "        self.list_file = list_file\n",
    "        self.num_segments = num_segments\n",
    "        self.new_length = new_length\n",
    "        self.modality = modality\n",
    "        self.image_tmpl = image_tmpl\n",
    "        self.transform = transform\n",
    "        self.random_shift = random_shift\n",
    "        self.test_mode = test_mode\n",
    "        \n",
    "        if self.modality == \"RGBDiff\":\n",
    "            self.new_length += 1\n",
    "            \n",
    "        self._parse_list()\n",
    "        \n",
    "    def _load_image(self, directory, idx):\n",
    "        if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
    "            return [Image.open(os.path.join(directory, self.image_tmp1.format(idx))).convert('RGB')]\n",
    "        elif self.modality == 'Flow':\n",
    "            x_img = Image.open(os.path.join(directory, self.image_tmpl.format('x', idx))).convert('L')\n",
    "            y_img = Image.open(os.path.join(directory, self.image_tmpl.format('y', idx))).convert('L')\n",
    "\n",
    "            return [x_img, y_img]\n",
    "\n",
    "    def _parse_list(self):\n",
    "        self.video_list = [VideoRecord(x.strip().split(' ')) for x in open(self.list_file)]\n",
    "\n",
    "    def _sample_indices(self, record):\n",
    "        \"\"\"\n",
    "        :param record: VideoRecord\n",
    "        :return: list\n",
    "        \"\"\"\n",
    "\n",
    "        average_duration = (record.num_frames - self.new_length + 1) // self.num_segments\n",
    "        if average_duration > 0:\n",
    "            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n",
    "        elif record.num_frames > self.num_segments:\n",
    "            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n",
    "        else:\n",
    "            offsets = np.zeros((self.num_segments,))\n",
    "        return offsets + 1\n",
    "\n",
    "    def _get_val_indices(self, record):\n",
    "        if record.num_frames > self.num_segments + self.new_length - 1:\n",
    "            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n",
    "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
    "        else:\n",
    "            offsets = np.zeros((self.num_segments,))\n",
    "        return offsets + 1\n",
    "\n",
    "    def _get_test_indices(self, record):\n",
    "\n",
    "        tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n",
    "\n",
    "        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
    "\n",
    "        return offsets + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.video_list[index]\n",
    "\n",
    "        if not self.test_mode:\n",
    "            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n",
    "        else:\n",
    "            segment_indices = self._get_test_indices(record)\n",
    "\n",
    "        return self.get(record, segment_indices)\n",
    "\n",
    "    def get(self, record, indices):\n",
    "\n",
    "        images = list()\n",
    "        for seg_ind in indices:\n",
    "            p = int(seg_ind)\n",
    "            for i in range(self.new_length):\n",
    "                seg_imgs = self._load_image(record.path, p)\n",
    "                images.extend(seg_imgs)\n",
    "                if p < record.num_frames:\n",
    "                    p += 1\n",
    "\n",
    "        process_data = self.transform(images)\n",
    "        return process_data, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "from dataset import TSNDataSet\n",
    "from models import TSN\n",
    "from transforms import *\n",
    "from opts import parser\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.dataset == 'ucf101':\n",
    "        num_class = 101\n",
    "    elif args.dataset == 'hmdb51':\n",
    "        num_class = 51\n",
    "    elif args.dataset == 'kinetics':\n",
    "        num_class = 400\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset '+args.dataset)\n",
    "\n",
    "    model = TSN(num_class, args.num_segments, args.modality,\n",
    "                base_model=args.arch,\n",
    "                consensus_type=args.consensus_type, dropout=args.dropout, partial_bn=not args.no_partialbn)\n",
    "\n",
    "    crop_size = model.crop_size\n",
    "    scale_size = model.scale_size\n",
    "    input_mean = model.input_mean\n",
    "    input_std = model.input_std\n",
    "    policies = model.get_optim_policies()\n",
    "    train_augmentation = model.get_augmentation()\n",
    "\n",
    "    model = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print((\"=> loading checkpoint '{}'\".format(args.resume)))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print((\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.evaluate, checkpoint['epoch'])))\n",
    "        else:\n",
    "            print((\"=> no checkpoint found at '{}'\".format(args.resume)))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    if args.modality != 'RGBDiff':\n",
    "        normalize = GroupNormalize(input_mean, input_std)\n",
    "    else:\n",
    "        normalize = IdentityTransform()\n",
    "\n",
    "    if args.modality == 'RGB':\n",
    "        data_length = 1\n",
    "    elif args.modality in ['Flow', 'RGBDiff']:\n",
    "        data_length = 5\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TSNDataSet(\"\", args.train_list, num_segments=args.num_segments,\n",
    "                   new_length=data_length,\n",
    "                   modality=args.modality,\n",
    "                   image_tmpl=\"img_{:05d}.jpg\" if args.modality in [\"RGB\", \"RGBDiff\"] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       train_augmentation,\n",
    "                       Stack(roll=args.arch == 'BNInception'),\n",
    "                       ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "                       normalize,\n",
    "                   ])),\n",
    "        batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TSNDataSet(\"\", args.val_list, num_segments=args.num_segments,\n",
    "                   new_length=data_length,\n",
    "                   modality=args.modality,\n",
    "                   image_tmpl=\"img_{:05d}.jpg\" if args.modality in [\"RGB\", \"RGBDiff\"] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "                   random_shift=False,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       GroupScale(int(scale_size)),\n",
    "                       GroupCenterCrop(crop_size),\n",
    "                       Stack(roll=args.arch == 'BNInception'),\n",
    "                       ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "                       normalize,\n",
    "                   ])),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    if args.loss_type == 'nll':\n",
    "        criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss type\")\n",
    "\n",
    "    for group in policies:\n",
    "        print(('group: {} has {} params, lr_mult: {}, decay_mult: {}'.format(\n",
    "            group['name'], len(group['params']), group['lr_mult'], group['decay_mult'])))\n",
    "\n",
    "    optimizer = torch.optim.SGD(policies,\n",
    "                                args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion, 0)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch, args.lr_steps)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:\n",
    "            prec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader))\n",
    "\n",
    "            # remember best prec@1 and save checkpoint\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "            }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    if args.no_partialbn:\n",
    "        model.module.partialBN(False)\n",
    "    else:\n",
    "        model.module.partialBN(True)\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if args.clip_gradient is not None:\n",
    "            total_norm = clip_grad_norm(model.parameters(), args.clip_gradient)\n",
    "            if total_norm > args.clip_gradient:\n",
    "                print(\"clipping gradient: {} with coef {}\".format(total_norm, args.clip_gradient / total_norm))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print(('Epoch: [{0}][{1}/{2}], lr: {lr:.5f}\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5, lr=optimizer.param_groups[-1]['lr'])))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, iter, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n",
    "\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print(('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5)))\n",
    "\n",
    "    print(('Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}'\n",
    "          .format(top1=top1, top5=top5, loss=losses)))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    filename = '_'.join((args.snapshot_pref, args.modality.lower(), filename))\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        best_name = '_'.join((args.snapshot_pref, args.modality.lower(), 'model_best.pth.tar'))\n",
    "        shutil.copyfile(filename, best_name)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr_steps):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    decay = 0.1 ** (sum(epoch >= np.array(lr_steps)))\n",
    "    lr = args.lr * decay\n",
    "    decay = args.weight_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr * param_group['lr_mult']\n",
    "        param_group['weight_decay'] = decay * param_group['decay_mult']\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from ops.basic_ops import ConsensusModule, Identity\n",
    "from transforms import *\n",
    "from torch.nn.init import normal, constant\n",
    "\n",
    "class TSN(nn.Module):\n",
    "    def __init__(self, num_class, num_segments, modality,\n",
    "                 base_model='resnet101', new_length=None,\n",
    "                 consensus_type='avg', before_softmax=True,\n",
    "                 dropout=0.8,\n",
    "                 crop_num=1, partial_bn=True):\n",
    "        super(TSN, self).__init__()\n",
    "        self.modality = modality\n",
    "        self.num_segments = num_segments\n",
    "        self.reshape = True\n",
    "        self.before_softmax = before_softmax\n",
    "        self.dropout = dropout\n",
    "        self.crop_num = crop_num\n",
    "        self.consensus_type = consensus_type\n",
    "        if not before_softmax and consensus_type != 'avg':\n",
    "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
    "\n",
    "        if new_length is None:\n",
    "            self.new_length = 1 if modality == \"RGB\" else 5\n",
    "        else:\n",
    "            self.new_length = new_length\n",
    "\n",
    "        print((\"\"\"\n",
    "Initializing TSN with base model: {}.\n",
    "TSN Configurations:\n",
    "    input_modality:     {}\n",
    "    num_segments:       {}\n",
    "    new_length:         {}\n",
    "    consensus_module:   {}\n",
    "    dropout_ratio:      {}\n",
    "        \"\"\".format(base_model, self.modality, self.num_segments, self.new_length, consensus_type, self.dropout)))\n",
    "\n",
    "        self._prepare_base_model(base_model)\n",
    "\n",
    "        feature_dim = self._prepare_tsn(num_class)\n",
    "\n",
    "        if self.modality == 'Flow':\n",
    "            print(\"Converting the ImageNet model to a flow init model\")\n",
    "            self.base_model = self._construct_flow_model(self.base_model)\n",
    "            print(\"Done. Flow model ready...\")\n",
    "        elif self.modality == 'RGBDiff':\n",
    "            print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
    "            self.base_model = self._construct_diff_model(self.base_model)\n",
    "            print(\"Done. RGBDiff model ready.\")\n",
    "\n",
    "        self.consensus = ConsensusModule(consensus_type)\n",
    "\n",
    "        if not self.before_softmax:\n",
    "            self.softmax = nn.Softmax()\n",
    "\n",
    "        self._enable_pbn = partial_bn\n",
    "        if partial_bn:\n",
    "            self.partialBN(True)\n",
    "\n",
    "    def _prepare_tsn(self, num_class):\n",
    "        feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
    "        if self.dropout == 0:\n",
    "            setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
    "            self.new_fc = None\n",
    "        else:\n",
    "            setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n",
    "            self.new_fc = nn.Linear(feature_dim, num_class)\n",
    "\n",
    "        std = 0.001\n",
    "        if self.new_fc is None:\n",
    "            normal(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n",
    "            constant(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n",
    "        else:\n",
    "            normal(self.new_fc.weight, 0, std)\n",
    "            constant(self.new_fc.bias, 0)\n",
    "        return feature_dim\n",
    "\n",
    "    def _prepare_base_model(self, base_model):\n",
    "\n",
    "        if 'resnet' in base_model or 'vgg' in base_model:\n",
    "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
    "            self.base_model.last_layer_name = 'fc'\n",
    "            self.input_size = 224\n",
    "            self.input_mean = [0.485, 0.456, 0.406]\n",
    "            self.input_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "            if self.modality == 'Flow':\n",
    "                self.input_mean = [0.5]\n",
    "                self.input_std = [np.mean(self.input_std)]\n",
    "            elif self.modality == 'RGBDiff':\n",
    "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
    "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
    "        elif base_model == 'BNInception':\n",
    "            import tf_model_zoo\n",
    "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
    "            self.base_model.last_layer_name = 'fc'\n",
    "            self.input_size = 224\n",
    "            self.input_mean = [104, 117, 128]\n",
    "            self.input_std = [1]\n",
    "\n",
    "            if self.modality == 'Flow':\n",
    "                self.input_mean = [128]\n",
    "            elif self.modality == 'RGBDiff':\n",
    "                self.input_mean = self.input_mean * (1 + self.new_length)\n",
    "\n",
    "        elif 'inception' in base_model:\n",
    "            import tf_model_zoo\n",
    "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
    "            self.base_model.last_layer_name = 'classif'\n",
    "            self.input_size = 299\n",
    "            self.input_mean = [0.5]\n",
    "            self.input_std = [0.5]\n",
    "        else:\n",
    "            raise ValueError('Unknown base model: {}'.format(base_model))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Override the default train() to freeze the BN parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TSN, self).train(mode)\n",
    "        count = 0\n",
    "        if self._enable_pbn:\n",
    "            print(\"Freezing BatchNorm2D except the first one.\")\n",
    "            for m in self.base_model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    count += 1\n",
    "                    if count >= (2 if self._enable_pbn else 1):\n",
    "                        m.eval()\n",
    "\n",
    "                        # shutdown update in frozen mode\n",
    "                        m.weight.requires_grad = False\n",
    "                        m.bias.requires_grad = False\n",
    "\n",
    "    def partialBN(self, enable):\n",
    "        self._enable_pbn = enable\n",
    "\n",
    "    def get_optim_policies(self):\n",
    "        first_conv_weight = []\n",
    "        first_conv_bias = []\n",
    "        normal_weight = []\n",
    "        normal_bias = []\n",
    "        bn = []\n",
    "\n",
    "        conv_cnt = 0\n",
    "        bn_cnt = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n",
    "                ps = list(m.parameters())\n",
    "                conv_cnt += 1\n",
    "                if conv_cnt == 1:\n",
    "                    first_conv_weight.append(ps[0])\n",
    "                    if len(ps) == 2:\n",
    "                        first_conv_bias.append(ps[1])\n",
    "                else:\n",
    "                    normal_weight.append(ps[0])\n",
    "                    if len(ps) == 2:\n",
    "                        normal_bias.append(ps[1])\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                ps = list(m.parameters())\n",
    "                normal_weight.append(ps[0])\n",
    "                if len(ps) == 2:\n",
    "                    normal_bias.append(ps[1])\n",
    "                  \n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                bn.extend(list(m.parameters()))\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                bn_cnt += 1\n",
    "                # later BN's are frozen\n",
    "                if not self._enable_pbn or bn_cnt == 1:\n",
    "                    bn.extend(list(m.parameters()))\n",
    "            elif len(m._modules) == 0:\n",
    "                if len(list(m.parameters())) > 0:\n",
    "                    raise ValueError(\"New atomic module type: {}. Need to give it a learning policy\".format(type(m)))\n",
    "\n",
    "        return [\n",
    "            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,\n",
    "             'name': \"first_conv_weight\"},\n",
    "            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,\n",
    "             'name': \"first_conv_bias\"},\n",
    "            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,\n",
    "             'name': \"normal_weight\"},\n",
    "            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,\n",
    "             'name': \"normal_bias\"},\n",
    "            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,\n",
    "             'name': \"BN scale/shift\"},\n",
    "        ]\n",
    "\n",
    "    def forward(self, input):\n",
    "        sample_len = (3 if self.modality == \"RGB\" else 2) * self.new_length\n",
    "\n",
    "        if self.modality == 'RGBDiff':\n",
    "            sample_len = 3 * self.new_length\n",
    "            input = self._get_diff(input)\n",
    "\n",
    "        base_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\n",
    "\n",
    "        if self.dropout > 0:\n",
    "            base_out = self.new_fc(base_out)\n",
    "\n",
    "        if not self.before_softmax:\n",
    "            base_out = self.softmax(base_out)\n",
    "        if self.reshape:\n",
    "            base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n",
    "\n",
    "        output = self.consensus(base_out)\n",
    "        return output.squeeze(1)\n",
    "\n",
    "    def _get_diff(self, input, keep_rgb=False):\n",
    "        input_c = 3 if self.modality in [\"RGB\", \"RGBDiff\"] else 2\n",
    "        input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])\n",
    "        if keep_rgb:\n",
    "            new_data = input_view.clone()\n",
    "        else:\n",
    "            new_data = input_view[:, :, 1:, :, :, :].clone()\n",
    "\n",
    "        for x in reversed(list(range(1, self.new_length + 1))):\n",
    "            if keep_rgb:\n",
    "                new_data[:, :, x, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
    "            else:\n",
    "                new_data[:, :, x - 1, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
    "\n",
    "        return new_data\n",
    "\n",
    "\n",
    "    def _construct_flow_model(self, base_model):\n",
    "        # modify the convolution layers\n",
    "        # Torch models are usually defined in a hierarchical way.\n",
    "        # nn.modules.children() return all sub modules in a DFS manner\n",
    "        modules = list(self.base_model.modules())\n",
    "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "        conv_layer = modules[first_conv_idx]\n",
    "        container = modules[first_conv_idx - 1]\n",
    "\n",
    "        # modify parameters, assume the first blob contains the convolution kernels\n",
    "        params = [x.clone() for x in conv_layer.parameters()]\n",
    "        kernel_size = params[0].size()\n",
    "        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n",
    "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "\n",
    "        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n",
    "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                             bias=True if len(params) == 2 else False)\n",
    "        new_conv.weight.data = new_kernels\n",
    "        if len(params) == 2:\n",
    "            new_conv.bias.data = params[1].data # add bias if neccessary\n",
    "        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n",
    "\n",
    "        # replace the first convlution layer\n",
    "        setattr(container, layer_name, new_conv)\n",
    "        return base_model\n",
    "\n",
    "    def _construct_diff_model(self, base_model, keep_rgb=False):\n",
    "        # modify the convolution layers\n",
    "        # Torch models are usually defined in a hierarchical way.\n",
    "        # nn.modules.children() return all sub modules in a DFS manner\n",
    "        modules = list(self.base_model.modules())\n",
    "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "        conv_layer = modules[first_conv_idx]\n",
    "        container = modules[first_conv_idx - 1]\n",
    "\n",
    "        # modify parameters, assume the first blob contains the convolution kernels\n",
    "        params = [x.clone() for x in conv_layer.parameters()]\n",
    "        kernel_size = params[0].size()\n",
    "        if not keep_rgb:\n",
    "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "        else:\n",
    "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n",
    "                                    1)\n",
    "            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n",
    "\n",
    "        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n",
    "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                             bias=True if len(params) == 2 else False)\n",
    "        new_conv.weight.data = new_kernels\n",
    "        if len(params) == 2:\n",
    "            new_conv.bias.data = params[1].data  # add bias if neccessary\n",
    "        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
    "\n",
    "        # replace the first convolution layer\n",
    "        setattr(container, layer_name, new_conv)\n",
    "        return base_model\n",
    "\n",
    "    @property\n",
    "    def crop_size(self):\n",
    "        return self.input_size\n",
    "\n",
    "    @property\n",
    "    def scale_size(self):\n",
    "        return self.input_size * 256 // 224\n",
    "\n",
    "    def get_augmentation(self):\n",
    "        if self.modality == 'RGB':\n",
    "            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n",
    "                                                   GroupRandomHorizontalFlip(is_flow=False)])\n",
    "        elif self.modality == 'Flow':\n",
    "            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
    "                                                   GroupRandomHorizontalFlip(is_flow=True)])\n",
    "        elif self.modality == 'RGBDiff':\n",
    "            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
    "                                                   GroupRandomHorizontalFlip(is_flow=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.basic_ops import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic_ops.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "class SegmentConsensus(torch.autograd.Function):\n",
    "\n",
    "    def __init__(self, consensus_type, dim=1):\n",
    "        self.consensus_type = consensus_type\n",
    "        self.dim = dim\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        self.shape = input_tensor.size()\n",
    "        if self.consensus_type == 'avg':\n",
    "            output = input_tensor.mean(dim=self.dim, keepdim=True)\n",
    "        elif self.consensus_type == 'identity':\n",
    "            output = input_tensor\n",
    "        else:\n",
    "            output = None\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.consensus_type == 'avg':\n",
    "            grad_in = grad_output.expand(self.shape) / float(self.shape[self.dim])\n",
    "        elif self.consensus_type == 'identity':\n",
    "            grad_in = grad_output\n",
    "        else:\n",
    "            grad_in = None\n",
    "\n",
    "        return grad_in\n",
    "\n",
    "\n",
    "class ConsensusModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, consensus_type, dim=1):\n",
    "        super(ConsensusModule, self).__init__()\n",
    "        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return SegmentConsensus(self.consensus_type, self.dim)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_grad_hook(name):\n",
    "    def hook(m, grad_in, grad_out):\n",
    "        print((name, grad_out[0].data.abs().mean(), grad_in[0].data.abs().mean()))\n",
    "        print((grad_out[0].size()))\n",
    "        print((grad_in[0].size()))\n",
    "\n",
    "        print((grad_out[0]))\n",
    "        print((grad_in[0]))\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    es = np.exp(scores - scores.max(axis=-1)[..., None])\n",
    "    return es / es.sum(axis=-1)[..., None]\n",
    "\n",
    "\n",
    "def log_add(log_a, log_b):\n",
    "    return log_a + np.log(1 + np.exp(log_b - log_a))\n",
    "\n",
    "\n",
    "def class_accuracy(prediction, label):\n",
    "    cf = confusion_matrix(prediction, label)\n",
    "    cls_cnt = cf.sum(axis=1)\n",
    "    cls_hit = np.diag(cf)\n",
    "\n",
    "    cls_acc = cls_hit / cls_cnt.astype(float)\n",
    "\n",
    "    mean_cls_acc = cls_acc.mean()\n",
    "\n",
    "    return cls_acc, mean_cls_acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch implementation of Temporal Segment Networks\")\n",
    "parser.add_argument('dataset', type=str, choices=['ucf101', 'hmdb51', 'kinetics'])\n",
    "parser.add_argument('modality', type=str, choices=['RGB', 'Flow', 'RGBDiff'])\n",
    "parser.add_argument('train_list', type=str)\n",
    "parser.add_argument('val_list', type=str)\n",
    "\n",
    "# ========================= Model Configs ==========================\n",
    "parser.add_argument('--arch', type=str, default=\"resnet101\")\n",
    "parser.add_argument('--num_segments', type=int, default=3)\n",
    "parser.add_argument('--consensus_type', type=str, default='avg',\n",
    "                    choices=['avg', 'max', 'topk', 'identity', 'rnn', 'cnn'])\n",
    "parser.add_argument('--k', type=int, default=3)\n",
    "\n",
    "parser.add_argument('--dropout', '--do', default=0.5, type=float,\n",
    "                    metavar='DO', help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--loss_type', type=str, default=\"nll\",\n",
    "                    choices=['nll'])\n",
    "\n",
    "# ========================= Learning Configs ==========================\n",
    "parser.add_argument('--epochs', default=45, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.001, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--lr_steps', default=[20, 40], type=float, nargs=\"+\",\n",
    "                    metavar='LRSteps', help='epochs to decay learning rate by 10')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 5e-4)')\n",
    "parser.add_argument('--clip-gradient', '--gd', default=None, type=float,\n",
    "                    metavar='W', help='gradient norm clipping (default: disabled)')\n",
    "parser.add_argument('--no_partialbn', '--npb', default=False, action=\"store_true\")\n",
    "\n",
    "# ========================= Monitor Configs ==========================\n",
    "parser.add_argument('--print-freq', '-p', default=20, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--eval-freq', '-ef', default=5, type=int,\n",
    "                    metavar='N', help='evaluation frequency (default: 5)')\n",
    "\n",
    "\n",
    "# ========================= Runtime Configs ==========================\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--snapshot_pref', type=str, default=\"\")\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--gpus', nargs='+', type=int, default=None)\n",
    "parser.add_argument('--flow_prefix', default=\"\", type=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from dataset import TSNDataSet\n",
    "from models import TSN\n",
    "from transforms import *\n",
    "from ops import ConsensusModule\n",
    "\n",
    "# options\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Standard video-level testing\")\n",
    "parser.add_argument('dataset', type=str, choices=['ucf101', 'hmdb51', 'kinetics'])\n",
    "parser.add_argument('modality', type=str, choices=['RGB', 'Flow', 'RGBDiff'])\n",
    "parser.add_argument('test_list', type=str)\n",
    "parser.add_argument('weights', type=str)\n",
    "parser.add_argument('--arch', type=str, default=\"resnet101\")\n",
    "parser.add_argument('--save_scores', type=str, default=None)\n",
    "parser.add_argument('--test_segments', type=int, default=25)\n",
    "parser.add_argument('--max_num', type=int, default=-1)\n",
    "parser.add_argument('--test_crops', type=int, default=10)\n",
    "parser.add_argument('--input_size', type=int, default=224)\n",
    "parser.add_argument('--crop_fusion_type', type=str, default='avg',\n",
    "                    choices=['avg', 'max', 'topk'])\n",
    "parser.add_argument('--k', type=int, default=3)\n",
    "parser.add_argument('--dropout', type=float, default=0.7)\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--gpus', nargs='+', type=int, default=None)\n",
    "parser.add_argument('--flow_prefix', type=str, default='')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "if args.dataset == 'ucf101':\n",
    "    num_class = 101\n",
    "elif args.dataset == 'hmdb51':\n",
    "    num_class = 51\n",
    "elif args.dataset == 'kinetics':\n",
    "    num_class = 400\n",
    "else:\n",
    "    raise ValueError('Unknown dataset '+args.dataset)\n",
    "\n",
    "net = TSN(num_class, 1, args.modality,\n",
    "          base_model=args.arch,\n",
    "          consensus_type=args.crop_fusion_type,\n",
    "          dropout=args.dropout)\n",
    "\n",
    "checkpoint = torch.load(args.weights)\n",
    "print(\"model epoch {} best prec@1: {}\".format(checkpoint['epoch'], checkpoint['best_prec1']))\n",
    "\n",
    "base_dict = {'.'.join(k.split('.')[1:]): v for k,v in list(checkpoint['state_dict'].items())}\n",
    "net.load_state_dict(base_dict)\n",
    "\n",
    "if args.test_crops == 1:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupScale(net.scale_size),\n",
    "        GroupCenterCrop(net.input_size),\n",
    "    ])\n",
    "elif args.test_crops == 10:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupOverSample(net.input_size, net.scale_size)\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(\"Only 1 and 10 crops are supported while we got {}\".format(args.test_crops))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        TSNDataSet(\"\", args.test_list, num_segments=args.test_segments,\n",
    "                   new_length=1 if args.modality == \"RGB\" else 5,\n",
    "                   modality=args.modality,\n",
    "                   image_tmpl=\"img_{:05d}.jpg\" if args.modality in ['RGB', 'RGBDiff'] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "                   test_mode=True,\n",
    "                   transform=torchvision.transforms.Compose([\n",
    "                       cropping,\n",
    "                       Stack(roll=args.arch == 'BNInception'),\n",
    "                       ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "                       GroupNormalize(net.input_mean, net.input_std),\n",
    "                   ])),\n",
    "        batch_size=1, shuffle=False,\n",
    "        num_workers=args.workers * 2, pin_memory=True)\n",
    "\n",
    "if args.gpus is not None:\n",
    "    devices = [args.gpus[i] for i in range(args.workers)]\n",
    "else:\n",
    "    devices = list(range(args.workers))\n",
    "\n",
    "\n",
    "net = torch.nn.DataParallel(net.cuda(devices[0]), device_ids=devices)\n",
    "net.eval()\n",
    "\n",
    "data_gen = enumerate(data_loader)\n",
    "\n",
    "total_num = len(data_loader.dataset)\n",
    "output = []\n",
    "\n",
    "\n",
    "def eval_video(video_data):\n",
    "    i, data, label = video_data\n",
    "    num_crop = args.test_crops\n",
    "\n",
    "    if args.modality == 'RGB':\n",
    "        length = 3\n",
    "    elif args.modality == 'Flow':\n",
    "        length = 10\n",
    "    elif args.modality == 'RGBDiff':\n",
    "        length = 18\n",
    "    else:\n",
    "        raise ValueError(\"Unknown modality \"+args.modality)\n",
    "\n",
    "    input_var = torch.autograd.Variable(data.view(-1, length, data.size(2), data.size(3)),\n",
    "                                        volatile=True)\n",
    "    rst = net(input_var).data.cpu().numpy().copy()\n",
    "    return i, rst.reshape((num_crop, args.test_segments, num_class)).mean(axis=0).reshape(\n",
    "        (args.test_segments, 1, num_class)\n",
    "    ), label[0]\n",
    "\n",
    "\n",
    "proc_start_time = time.time()\n",
    "max_num = args.max_num if args.max_num > 0 else len(data_loader.dataset)\n",
    "\n",
    "for i, (data, label) in data_gen:\n",
    "    if i >= max_num:\n",
    "        break\n",
    "    rst = eval_video((i, data, label))\n",
    "    output.append(rst[1:])\n",
    "    cnt_time = time.time() - proc_start_time\n",
    "    print('video {} done, total {}/{}, average {} sec/video'.format(i, i+1,\n",
    "                                                                    total_num,\n",
    "                                                                    float(cnt_time) / (i+1)))\n",
    "\n",
    "video_pred = [np.argmax(np.mean(x[0], axis=0)) for x in output]\n",
    "\n",
    "video_labels = [x[1] for x in output]\n",
    "\n",
    "\n",
    "cf = confusion_matrix(video_labels, video_pred).astype(float)\n",
    "\n",
    "cls_cnt = cf.sum(axis=1)\n",
    "cls_hit = np.diag(cf)\n",
    "\n",
    "cls_acc = cls_hit / cls_cnt\n",
    "\n",
    "print(cls_acc)\n",
    "\n",
    "print('Accuracy {:.02f}%'.format(np.mean(cls_acc) * 100))\n",
    "\n",
    "if args.save_scores is not None:\n",
    "\n",
    "    # reorder before saving\n",
    "    name_list = [x.strip().split()[0] for x in open(args.test_list)]\n",
    "\n",
    "    order_dict = {e:i for i, e in enumerate(sorted(name_list))}\n",
    "\n",
    "    reorder_output = [None] * len(output)\n",
    "    reorder_label = [None] * len(output)\n",
    "\n",
    "    for i in range(len(output)):\n",
    "        idx = order_dict[name_list[i]]\n",
    "        reorder_output[idx] = output[i]\n",
    "        reorder_label[idx] = video_labels[i]\n",
    "\n",
    "    np.savez(args.save_scores, scores=reorder_output, labels=reorder_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import numbers\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "class GroupRandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "\n",
    "        w, h = img_group[0].size\n",
    "        th, tw = self.size\n",
    "\n",
    "        out_images = list()\n",
    "\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "\n",
    "        for img in img_group:\n",
    "            assert(img.size[0] == w and img.size[1] == h)\n",
    "            if w == tw and h == th:\n",
    "                out_images.append(img)\n",
    "            else:\n",
    "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
    "\n",
    "        return out_images\n",
    "\n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "class GroupRandomHorizontalFlip(object):\n",
    "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, is_flow=False):\n",
    "        self.is_flow = is_flow\n",
    "\n",
    "    def __call__(self, img_group, is_flow=False):\n",
    "        v = random.random()\n",
    "        if v < 0.5:\n",
    "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
    "            if self.is_flow:\n",
    "                for i in range(0, len(ret), 2):\n",
    "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
    "            return ret\n",
    "        else:\n",
    "            return img_group\n",
    "\n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
    "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
    "\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
    "            t.sub_(m).div_(s)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class GroupScale(object):\n",
    "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
    "    'size' will be the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Scale(size, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "class GroupOverSample(object):\n",
    "    def __init__(self, crop_size, scale_size=None):\n",
    "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
    "\n",
    "        if scale_size is not None:\n",
    "            self.scale_worker = GroupScale(scale_size)\n",
    "        else:\n",
    "            self.scale_worker = None\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "\n",
    "        if self.scale_worker is not None:\n",
    "            img_group = self.scale_worker(img_group)\n",
    "\n",
    "        image_w, image_h = img_group[0].size\n",
    "        crop_w, crop_h = self.crop_size\n",
    "\n",
    "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
    "        oversample_group = list()\n",
    "        for o_w, o_h in offsets:\n",
    "            normal_group = list()\n",
    "            flip_group = list()\n",
    "            for i, img in enumerate(img_group):\n",
    "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
    "                normal_group.append(crop)\n",
    "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "                if img.mode == 'L' and i % 2 == 0:\n",
    "                    flip_group.append(ImageOps.invert(flip_crop))\n",
    "                else:\n",
    "                    flip_group.append(flip_crop)\n",
    "\n",
    "            oversample_group.extend(normal_group)\n",
    "            oversample_group.extend(flip_group)\n",
    "        return oversample_group\n",
    "\n",
    "\n",
    "class GroupMultiScaleCrop(object):\n",
    "\n",
    "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
    "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
    "        self.max_distort = max_distort\n",
    "        self.fix_crop = fix_crop\n",
    "        self.more_fix_crop = more_fix_crop\n",
    "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
    "        self.interpolation = Image.BILINEAR\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "\n",
    "        im_size = img_group[0].size\n",
    "\n",
    "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
    "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
    "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
    "                         for img in crop_img_group]\n",
    "        return ret_img_group\n",
    "\n",
    "    def _sample_crop_size(self, im_size):\n",
    "        image_w, image_h = im_size[0], im_size[1]\n",
    "\n",
    "        # find a crop size\n",
    "        base_size = min(image_w, image_h)\n",
    "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
    "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
    "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
    "\n",
    "        pairs = []\n",
    "        for i, h in enumerate(crop_h):\n",
    "            for j, w in enumerate(crop_w):\n",
    "                if abs(i - j) <= self.max_distort:\n",
    "                    pairs.append((w, h))\n",
    "\n",
    "        crop_pair = random.choice(pairs)\n",
    "        if not self.fix_crop:\n",
    "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
    "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
    "        else:\n",
    "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
    "\n",
    "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
    "\n",
    "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
    "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
    "        return random.choice(offsets)\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
    "        w_step = (image_w - crop_w) // 4\n",
    "        h_step = (image_h - crop_h) // 4\n",
    "\n",
    "        ret = list()\n",
    "        ret.append((0, 0))  # upper left\n",
    "        ret.append((4 * w_step, 0))  # upper right\n",
    "        ret.append((0, 4 * h_step))  # lower left\n",
    "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
    "        ret.append((2 * w_step, 2 * h_step))  # center\n",
    "\n",
    "        if more_fix_crop:\n",
    "            ret.append((0, 2 * h_step))  # center left\n",
    "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
    "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
    "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
    "\n",
    "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
    "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
    "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
    "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class GroupRandomSizedCrop(object):\n",
    "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
    "    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n",
    "    This is popularly used to train the Inception networks\n",
    "    size: size of the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        for attempt in range(10):\n",
    "            area = img_group[0].size[0] * img_group[0].size[1]\n",
    "            target_area = random.uniform(0.08, 1.0) * area\n",
    "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                w, h = h, w\n",
    "\n",
    "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
    "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
    "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
    "                found = True\n",
    "                break\n",
    "        else:\n",
    "            found = False\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "\n",
    "        if found:\n",
    "            out_group = list()\n",
    "            for img in img_group:\n",
    "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
    "                assert(img.size == (w, h))\n",
    "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
    "            return out_group\n",
    "        else:\n",
    "            # Fallback\n",
    "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
    "            crop = GroupRandomCrop(self.size)\n",
    "            return crop(scale(img_group))\n",
    "\n",
    "\n",
    "class Stack(object):\n",
    "\n",
    "    def __init__(self, roll=False):\n",
    "        self.roll = roll\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if img_group[0].mode == 'L':\n",
    "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
    "        elif img_group[0].mode == 'RGB':\n",
    "            if self.roll:\n",
    "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
    "            else:\n",
    "                return np.concatenate(img_group, axis=2)\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "    def __init__(self, div=True):\n",
    "        self.div = div\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
    "        else:\n",
    "            # handle PIL Image\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
    "            # put it from HWC to CHW format\n",
    "            # yikes, this transpose takes 80% of the loading time/CPU\n",
    "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        return img.float().div(255) if self.div else img.float()\n",
    "\n",
    "\n",
    "class IdentityTransform(object):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trans = torchvision.transforms.Compose([\n",
    "        GroupScale(256),\n",
    "        GroupRandomCrop(224),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(\n",
    "            mean=[.485, .456, .406],\n",
    "            std=[.229, .224, .225]\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    im = Image.open('../tensorflow-model-zoo.torch/lena_299.png')\n",
    "\n",
    "    color_group = [im] * 3\n",
    "    rst = trans(color_group)\n",
    "\n",
    "    gray_group = [im.convert('L')] * 9\n",
    "    gray_rst = trans(gray_group)\n",
    "\n",
    "    trans2 = torchvision.transforms.Compose([\n",
    "        GroupRandomSizedCrop(256),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(\n",
    "            mean=[.485, .456, .406],\n",
    "            std=[.229, .224, .225])\n",
    "    ])\n",
    "    print(trans2(color_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
