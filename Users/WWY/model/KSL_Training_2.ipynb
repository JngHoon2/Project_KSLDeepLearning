{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KSL_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9tPfq7SwaOO"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTly8jUwIx3",
        "outputId": "d75b056d-f5ad-4a2a-9744-dc0f60042661"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG9bSyNzxbTt",
        "outputId": "c6641c9e-65b0-4432-baac-16850963db98"
      },
      "source": [
        "# 만약에 파일 없으면 !mkdir \"KSLProject\"\n",
        "\n",
        "# %cd 'MyDrive'\n",
        "# %cd 'KSLProject'\n",
        "# %cd 'hmdb51'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/KSLProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JGUP4CTzCgr",
        "outputId": "340f83b5-741d-4700-a813-2caf9f573e93"
      },
      "source": [
        "!wget http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-19 18:22:59--  http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
            "Resolving serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)... 128.148.254.114\n",
            "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar [following]\n",
            "--2021-05-19 18:22:59--  https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\n",
            "Connecting to serre-lab.clps.brown.edu (serre-lab.clps.brown.edu)|128.148.254.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2124008126 (2.0G)\n",
            "Saving to: ‘hmdb51_org.rar’\n",
            "\n",
            "hmdb51_org.rar      100%[===================>]   1.98G  19.8MB/s    in 74s     \n",
            "\n",
            "2021-05-19 18:24:13 (27.4 MB/s) - ‘hmdb51_org.rar’ saved [2124008126/2124008126]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y97dkozn348n",
        "outputId": "68bfe2ad-b2b1-476d-9d27-1c548f415b2e"
      },
      "source": [
        "# !mkdir 'hmdb51'\n",
        "%cd 'hmdb51'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/KSLProject/hmdb51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWbvLYqk39QA"
      },
      "source": [
        "#unzip rar file\n",
        "!unrar x ../hmdb51_org.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmT9xwcX0lv8"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyb2c_eD3CYU"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okwzROh049Oq"
      },
      "source": [
        "!unrar x for i in brush_hair.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdlCKIU76e54"
      },
      "source": [
        "file_list = []\n",
        "file_list = os.listdir(os.getcwd())\n",
        "print(file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d00KNZJj7Xoq"
      },
      "source": [
        "cur_path = os.getcwd() \n",
        "cur_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7AKAqJ65fY"
      },
      "source": [
        "# for x in file_list:\n",
        "#   !unrar x cur_path\n",
        "!unrar x for i in file_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TANcp2Y48Qmk"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJK2ZDVE8V7H"
      },
      "source": [
        "frames = 15\n",
        "Width = 256\n",
        "Height = 256"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGyHzRW8f06"
      },
      "source": [
        "#비디오명과 레이블 짝을 가져온다.\n",
        "def load_video_names(path):\n",
        "\n",
        "  videos = []\n",
        "  labels = []\n",
        "  for category in os.listdir(path): #root 폴더의 각 하위폴더들은 클래스이름이기 때문에 이것을 category로 저장\n",
        "    for video in os.listdir(path+\"/\"+category): #인자로 받은 경로(path)와 클래스명을 붙여서 각각의 비디오별\n",
        "      videos.append(path+\"/\"+category+\"/\"+video) #비디오의 경로명을 videos에 넣는다.\n",
        "      labels.append(category) #category명 즉 폴더명이 클래스이므로 label로 넣는다.\n",
        "  return np.array(videos), np.array(labels) #이것들을 array값으로 반환한다.\n",
        "\n",
        "#프레임 전처리 함수 preprocess()\n",
        "def preprocess(frame):\n",
        "  frame = cv2.resize(frame, (Width, Height))\n",
        "  # 프레임 픽셀 -1에서 1 범위로 정규화(normalization)\n",
        "  frame = frame - 127.5 # RGB값 : 255/2 - 0.5\n",
        "  frame = frame / 127.5\n",
        "  return frame\n",
        "\n",
        "#비디오 불러오기 함수 load_video()\n",
        "def load_video(video_path):\n",
        "  video_frames = []\n",
        "  cap = cv2.VideoCapture(video_path) #비디오 경로를 입력하여 cap 인자로 cv2비디오캡처를 실행\n",
        "  while True:\n",
        "    ret, frame = cap.read() #비디오의 캡처를 읽어오고\n",
        "    if ret == True: #실행되는 동안\n",
        "      video_frames.append(preprocess(frame)) #전처리된 프레임을 video_frames 리스트에 추가\n",
        "      break\n",
        "  cap.release()\n",
        "  video_frames = select_frames(video_frames) #비디오 프레임을 select frames()에 처리시킨다.\n",
        "  if len(video_frames) != frames:\n",
        "    print('short_video ',video_path, len(video_frames))\n",
        "    \n",
        "  return np.array(video_frames)\n",
        "\n",
        "\n",
        "def select_frames(video_frames): # video_frames인자는 전처리한 프레임들을 저장한 리스트 형태\n",
        "  selected_frames = []\n",
        "  if len(video_frames) > frames: # 전체 비디오 프레임 수가 지정한 프레임 (15)보다 큰 동안\n",
        "    fn = len(video_frames)//frames # 전체 프레임수를 지정프레임수로 나누고 나머지를 버린 몫을 fn으로 산정 ( //연산자 : 나눈 값에서 소수점을 버린 정수값을 취함)\n",
        "    # 예를들어 가져온 비디오의 전체프레임이 119프레임이라할때 15로 나누면 7.93333 이고 여기에 나머지를 버리면 7이라는 값이 나옴\n",
        "    # 즉, 전체프레임 중에서 15프레임 단위로 분할하여 선택적으로 프레임을 가져오기 위해서 설정\n",
        "    f_num = 0 # 현재 선택된 프레임의 수 \n",
        "    for f in video_frames: # 각각의 비디오 프레임 값(array배열 값)에 대해서 f로 지정\n",
        "      if len(selected_frames) < frames: # 만약 선택된 프레임의 수가 전체 할당하고자하는 프레임 수(15)보다 작은 상태라면\n",
        "        if f_num % fn == 0: # f_num이 fn의 배수가 될 때마다(나누어 떨어질 때마다) 해당 프레임의 위치를 표시\n",
        "          selected_frames.append(f) # selected_frames에 프레임을 추가\n",
        "          #즉 fn이 7값이면 7프레임 단위로 끊어서 전체 비디오의 프레임을 가져온다. 7 * 15 = 105, 7 * 16 = \n",
        "      f_num += 1 # 선택된 프레임 수 1 증가\n",
        "  else:\n",
        "    selected_frames = video_frames #만약 전체 비디오의 프레임 수가 지정한 프레임(15)보다 작은경우 그냥 그프레임 자체를 데이터로 설정\n",
        "  return selected_frames  #즉, 전체 프레임을 설정한 frames=15라는 값에 의해 15개의 프레임만 선택적으로 균일한 간격을 이루어 선택적으로 취하는 코드\n",
        "\n",
        "\n",
        "# X, y = create_dataset(videos, encoded_labels, selected_indexes)\n",
        "def create_dataset(videos, labels, indx):\n",
        "  X = []\n",
        "  y = []\n",
        "  for video, label in zip(videos[indx], labels[indx]):\n",
        "    X.append(load_video(video))\n",
        "    y.append(label)\n",
        "\n",
        "  return np.array(X), np.array(y)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_wU1ALQ_grD"
      },
      "source": [
        "videos, labels = load_video_names('./hmdb51') #path ./hmdb51 내에 있는 각각의 클래스 폴더들의 비디오 이름들을 가져와 videos와 labels로 할당한다.\n",
        "samples = len(videos)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYRYASi2EnGE",
        "outputId": "ebfb7ef2-dec9-46d8-ce48-46b2ce7148f2"
      },
      "source": [
        "samples\n",
        "# print(labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1289"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7GrM53q_zl-"
      },
      "source": [
        "#모든 비디오의 label값이 중복으로 나타나는 것을\n",
        "#np.unique()함수를 이용하여 같은값의 중복을 없앰\n",
        "classes = len(np.unique(labels)) \n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaOoVRvr_3Dt",
        "outputId": "ab6904e2-d1fb-44df-8df9-952b3605a37f"
      },
      "source": [
        "classes"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhgQ03VU_6Vl",
        "outputId": "9e781fb2-eaca-444d-81f1-4b823fa6da74"
      },
      "source": [
        "# print(np.unique(labels, return_counts=True))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array(['나', '만나다', '보다', '비빔밥', '안녕하세요', '얼굴'],\n",
            "      dtype='<U12'), array([19, 20, 17, 20, 20, 19]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcA-BUl-E_tL",
        "outputId": "605800b6-6ed5-4bc4-b801-529990b9de5e"
      },
      "source": [
        "#비디오가 제대로 불러와졌는지 확인\n",
        "\n",
        "labels_counts = np.unique(labels, return_counts=True) #각 레이블이 몇 개씩 존재하는지 반환하는 인자 return_counts\n",
        "print(labels_counts)\n",
        "print(np.shape(labels_counts))\n",
        "for l, n in zip(labels_counts[0], labels_counts[1]): #l과 n에 각각 첫번째 배열의 요소와 두번째 배열의 요소를 대응\n",
        "# l값은 분류 레이블의 이름이 저장되어있고 n값은 해당 레이블의 개수가 저장되어있음\n",
        " print(l,n)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array(['가렵다', '가슴', '고열', '골절', '너무 아파요', '눈',\n",
            "       '다리', '두드러기생기다', '뒤', '뒤통수', '등', '머리', '목',\n",
            "       '무릎', '밑에', '발', '발가락', '발목', '발작', '복부',\n",
            "       '복통', '볼', '부러지다', '붕대', '뼈', '삼키다', '손',\n",
            "       '손가락', '손목', '심장마비', '쓰러지다', '앞', '어깨',\n",
            "       '어지러움', '얼굴', '열나다', '옆쪽', '오른쪽', '왼쪽',\n",
            "       '위에', '응급처리', '이마', '이물질', '인대', '자상',\n",
            "       '절단', '지혈대', '진통제', '질식', '체온계', '출혈',\n",
            "       '코', '탈골', '토하다', '팔', '팔꿈치', '피나다',\n",
            "       '함몰되다', '해독제', '해열제', '허리', '허벅지',\n",
            "       '호흡곤란', '호흡기', '화상'], dtype='<U15'), array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
            "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
            "       20, 20, 20, 20, 20, 19, 20, 20, 20, 11, 20, 20, 20, 20, 19, 20, 20,\n",
            "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]))\n",
            "(2, 65)\n",
            "가렵다 20\n",
            "가슴 20\n",
            "고열 20\n",
            "골절 20\n",
            "너무 아파요 20\n",
            "눈 20\n",
            "다리 20\n",
            "두드러기생기다 20\n",
            "뒤 20\n",
            "뒤통수 20\n",
            "등 20\n",
            "머리 20\n",
            "목 20\n",
            "무릎 20\n",
            "밑에 20\n",
            "발 20\n",
            "발가락 20\n",
            "발목 20\n",
            "발작 20\n",
            "복부 20\n",
            "복통 20\n",
            "볼 20\n",
            "부러지다 20\n",
            "붕대 20\n",
            "뼈 20\n",
            "삼키다 20\n",
            "손 20\n",
            "손가락 20\n",
            "손목 20\n",
            "심장마비 20\n",
            "쓰러지다 20\n",
            "앞 20\n",
            "어깨 20\n",
            "어지러움 20\n",
            "얼굴 20\n",
            "열나다 20\n",
            "옆쪽 20\n",
            "오른쪽 20\n",
            "왼쪽 20\n",
            "위에 19\n",
            "응급처리 20\n",
            "이마 20\n",
            "이물질 20\n",
            "인대 11\n",
            "자상 20\n",
            "절단 20\n",
            "지혈대 20\n",
            "진통제 20\n",
            "질식 19\n",
            "체온계 20\n",
            "출혈 20\n",
            "코 20\n",
            "탈골 20\n",
            "토하다 20\n",
            "팔 20\n",
            "팔꿈치 20\n",
            "피나다 20\n",
            "함몰되다 20\n",
            "해독제 20\n",
            "해열제 20\n",
            "허리 20\n",
            "허벅지 20\n",
            "호흡곤란 20\n",
            "호흡기 20\n",
            "화상 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDXKUOu3FRxR",
        "outputId": "417c3429-3c79-49a6-8698-995a758de140"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "le = LabelEncoder() #LabelEncoder() 객체를 통해 문자로 이루어진 클래스를 숫자 형태로 적용\n",
        "le.fit(np.unique(labels)) #label의 유니크 값을 le.fit()함수에 넣어 \n",
        "\n",
        "encoded_labels = le.transform(labels) #fit을 시켜주지 않으면 다음과 같은 에러메세지를 출력 \n",
        "# This LabelEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
        "# print(encoded_labels) # 결과값 : [63 63 63 ... 2  2  2]\n",
        "encoded_labels = np.reshape(encoded_labels, (-1,1)) # 하나의 row에 값들을 나열 (수평축의 크기를 1로 해서 1개의 요소가 들어가면 다음 열로 넘어감)\n",
        "# print(encoded_labels)\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(encoded_labels) # 나열된 레이블 값들을 원핫 인코더에 적용시키기 위해 .fit()실행\n",
        "encoded_labels = encoder.transform(encoded_labels)\n",
        "\n",
        "print(encoded_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 55)\t1.0\n",
            "  (1, 55)\t1.0\n",
            "  (2, 55)\t1.0\n",
            "  (3, 55)\t1.0\n",
            "  (4, 55)\t1.0\n",
            "  (5, 55)\t1.0\n",
            "  (6, 55)\t1.0\n",
            "  (7, 55)\t1.0\n",
            "  (8, 55)\t1.0\n",
            "  (9, 55)\t1.0\n",
            "  (10, 55)\t1.0\n",
            "  (11, 55)\t1.0\n",
            "  (12, 55)\t1.0\n",
            "  (13, 55)\t1.0\n",
            "  (14, 55)\t1.0\n",
            "  (15, 55)\t1.0\n",
            "  (16, 55)\t1.0\n",
            "  (17, 55)\t1.0\n",
            "  (18, 55)\t1.0\n",
            "  (19, 55)\t1.0\n",
            "  (20, 54)\t1.0\n",
            "  (21, 54)\t1.0\n",
            "  (22, 54)\t1.0\n",
            "  (23, 54)\t1.0\n",
            "  (24, 54)\t1.0\n",
            "  :\t:\n",
            "  (1264, 57)\t1.0\n",
            "  (1265, 57)\t1.0\n",
            "  (1266, 57)\t1.0\n",
            "  (1267, 57)\t1.0\n",
            "  (1268, 57)\t1.0\n",
            "  (1269, 56)\t1.0\n",
            "  (1270, 56)\t1.0\n",
            "  (1271, 56)\t1.0\n",
            "  (1272, 56)\t1.0\n",
            "  (1273, 56)\t1.0\n",
            "  (1274, 56)\t1.0\n",
            "  (1275, 56)\t1.0\n",
            "  (1276, 56)\t1.0\n",
            "  (1277, 56)\t1.0\n",
            "  (1278, 56)\t1.0\n",
            "  (1279, 56)\t1.0\n",
            "  (1280, 56)\t1.0\n",
            "  (1281, 56)\t1.0\n",
            "  (1282, 56)\t1.0\n",
            "  (1283, 56)\t1.0\n",
            "  (1284, 56)\t1.0\n",
            "  (1285, 56)\t1.0\n",
            "  (1286, 56)\t1.0\n",
            "  (1287, 56)\t1.0\n",
            "  (1288, 56)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmRhB0RRFubo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8277f9ce-8ee2-4030-9173-f3f9c63e4ed5"
      },
      "source": [
        "# 인코딩 레이블 값을 array값으로 covert\n",
        "encoded_labels = encoded_labels.toarray()\n",
        "print(encoded_labels)\n",
        "#어쨋든 데이터로 들어가야 하는 레이블 값은 다음과 같은 값이어야 함"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59pOqgEcLZXA",
        "outputId": "2f593d78-7918-4ae7-9904-a4f9aeff1a98"
      },
      "source": [
        "np.shape(encoded_labels) #(영상 개수, 클래스 개수)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1289, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh3NKLlUF3PF"
      },
      "source": [
        "#케라스 모델 생성\n",
        "from keras.models import Input, Model\n",
        "from keras.layers import TimeDistributed, LSTM\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, LeakyReLU, BatchNormalization\n",
        "from keras.layers import Dense, Flatten, GlobalMaxPooling2D\n",
        "from keras.layers import MaxPooling3D\n",
        "from keras.layers import concatenate\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMQjyNHeL4mp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC4YSY0uGcC1"
      },
      "source": [
        "def res_block(model, filters):\n",
        "  '''\n",
        "  feature extraction 하기 위한 residual block\n",
        "  1개의 CNN 레이어와 leaky relu action 함수 포함\n",
        "  '''\n",
        "\n",
        "  #model = TimeDistributed(Conv2D(filters=filters,\n",
        "  #kernel_size=3, padding='same'))(model)\n",
        "  start_block = model\n",
        "  model = Conv2D(filters=filters, kernel_size=3, padding='same')(model)\n",
        "  model = BatchNormalization(momentum=0.9)(model)\n",
        "  model = LeakyReLU(0.2)(model)\n",
        "  return concatenate([start_block, model])\n",
        "\n",
        "def create_model():\n",
        "  '''\n",
        "  초기모델은 ConvLSTM2D 레이어를 구성\n",
        "  이를 통해 비디오의 spatial(공간적)특성과 temporal(일시성, 시간성)특성을 캐치\n",
        "  '''\n",
        "  input_layer = Input(shape=(frames, Width,Height,3))\n",
        "\n",
        "  model = ConvLSTM2D(32, 3 , padding='same', return_sequences=False)(input_layer)\n",
        "  model = BatchNormalization(momentum=0.9)(model)\n",
        "  model = LeakyReLU(0.2)(model)\n",
        "\n",
        "  filters = 64 #필터 개수 설정\n",
        "\n",
        "  for _ in range(6):\n",
        "    model = res_block(model, filters)\n",
        "    try:\n",
        "      model = MaxPooling3D((2,2,2))(model)\n",
        "    except:\n",
        "      model = MaxPooling2D((2,2))(model)\n",
        "    if filters < 512:\n",
        "      filters *= 2\n",
        "\n",
        "  #model = TimeDistributed(Conv2D(filters, 3 , padding='same'))(model)\n",
        "  #model = TimeDistributed(GlobalMaxPooling2D())(model)\n",
        "  model = Flatten()(model)\n",
        "\n",
        "  # 출력층\n",
        "  model = Dense(classes, activation='softmax')(model)\n",
        "  model = Model(input_layer , model)\n",
        "  model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pma6KdknIbKA"
      },
      "source": [
        "classifier = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wawRWRuHIeP4",
        "outputId": "80a1f03f-b742-4cd3-bd9c-17ab9c5a2017"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 15, 256, 256 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d (ConvLSTM2D)       (None, 256, 256, 32) 40448       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 256, 256, 32) 128         conv_lst_m2d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 256, 256, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 256, 64) 18496       leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 256, 256, 64) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256, 256, 96) 0           leaky_re_lu[0][0]                \n",
            "                                                                 leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 128, 96) 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 128 110720      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 128 512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 128 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 128, 128, 224 0           max_pooling2d[0][0]              \n",
            "                                                                 leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 224)  0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 256)  516352      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 64, 256)  1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 256)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 480)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 480)  0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 512)  2212352     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 512)  2048        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 512)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 992)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 992)  0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 512)  4571648     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 512)  2048        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 16, 1504) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 1504)   0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    6930944     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 8, 8, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 8, 8, 2016)   0           max_pooling2d_4[0][0]            \n",
            "                                                                 leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 2016)   0           concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 32256)        0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 6)            193542      flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 14,602,566\n",
            "Trainable params: 14,598,534\n",
            "Non-trainable params: 4,032\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFaJdlHqIhjo"
      },
      "source": [
        "# 모델 그래프 생성\n",
        "from keras.utils import plot_model\n",
        "plot_model(classifier)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcsx10UbI24Z"
      },
      "source": [
        "#모델 저장할 경로 설정\n",
        "model_name = './conv_lstm_2d.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgs2d9rYI9oE"
      },
      "source": [
        "# pre-train된 가중치를 불러오기\n",
        "try:\n",
        "  classifier.load_weights(model_name)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "BbfPCGPBJKH0",
        "outputId": "35c83a9a-d34a-4a31-d9d8-0966e9a98050"
      },
      "source": [
        "batch_size = 4 #batch 사이즈 설정\n",
        "# total batches\n",
        "\n",
        "# samples//batch_size = 1289 // 4  = 322 < 1289/4 = 322.25 \n",
        "if samples//batch_size < samples/batch_size: # \n",
        "  batches = (samples//batch_size)+1 # 323\n",
        "else:\n",
        "  batches = samples//batch_size \n",
        "\n",
        "#model.fit() 대신에 train_on_batch()를 사용하는 이유는 각 에폭마다 가중치의 업데이트가 매번 일어나는 것이 아니라\n",
        "# 특정 주기에만 업데이트를 시켜주기 위해서 이러한 방법을 사용하는 것이다.\n",
        "# 또한 각 배치 사이에서 값의 조정이 가능하다.\n",
        "\n",
        "for e in range(100): #에폭 수 \n",
        "  index = list(range(samples))\n",
        "  np.random.shuffle(index) # 인덱스를 저장한 배열을 무작위 순서로 설정\n",
        "  accuracy = 0\n",
        "  for batch in range(batches): # 0 ~ 322\n",
        "    bs = batch*batch_size # MINIMUM = 0 * 4 = 0 ~ MAXIMUM = (322) * 4 =  # 0 ~ 1288\n",
        "    be = bs+batch_size # MIN = 0 * 4 ~ MAX = 1288 * 4 = 5152\n",
        "    selected_indexes = index[bs:be] #index [0:1289, 0:5153]\n",
        "\n",
        "    X, y = create_dataset(videos, encoded_labels, selected_indexes)\n",
        "    results = classifier.train_on_batch(X,y)\n",
        "    print('\\r', batch, '/', batches, ' : ', results[0], results[1], end='')\n",
        "    accuracy += results[1]\n",
        "    if batch%100 == 0:\n",
        "      classifier.save_weights(model_name)\n",
        "  print('\\r> ',e,', Accuracy = ', accuracy/batches)\n",
        "  classifier.save_weights(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">  0 , Accuracy =  0.10344827586206896\n",
            ">  1 , Accuracy =  0.10344827586206896\n",
            ">  2 , Accuracy =  0.15804597735404968\n",
            ">  3 , Accuracy =  0.23275862068965517\n",
            ">  4 , Accuracy =  0.22413793103448276\n",
            ">  5 , Accuracy =  0.32183908080232554\n",
            ">  6 , Accuracy =  0.41091954091499594\n",
            ">  7 , Accuracy =  0.3620689655172414\n",
            ">  8 , Accuracy =  0.38218390838853245\n",
            ">  9 , Accuracy =  0.4712643685012028\n",
            ">  10 , Accuracy =  0.44252873597473935\n",
            ">  11 , Accuracy =  0.4856321842506014\n",
            ">  12 , Accuracy =  0.5747126443632717\n",
            ">  13 , Accuracy =  0.5431034482758621\n",
            " 4 / 29  :  0.6163866519927979 0.5"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4713aa7e9c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mselected_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2e364423a97d>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(videos, labels, indx)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2e364423a97d>\u001b[0m in \u001b[0;36mload_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mvideo_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}